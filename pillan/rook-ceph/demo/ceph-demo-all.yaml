---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  # clusterID is the namespace where operator is deployed.
  clusterID: rook-ceph

  # CephFS filesystem name into which the volume shall be created
  fsName: scratch

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: scratch-data0

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
reclaimPolicy: Delete
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-demo-all-pvc1
spec:
  storageClassName: rook-ceph-block
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-demo-all-pvc2
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 200Gi
  storageClassName: rook-cephfs
---
apiVersion: v1
kind: Pod
metadata:
  name: ceph-demo-all
  labels:
    app: ceph-demo-all
spec:
  containers:
    - name: alpine
      image: alpine
      command: ["/bin/sh", "-c", "trap : TERM INT; sleep infinity & wait"]
      volumeMounts:
        - name: jhome
          mountPath: /data/jhome
        - name: lsstdata
          mountPath: /data/lsstdata
        - name: project
          mountPath: /data/project
        - name: scratch
          mountPath: /data/scratch
        - name: pvc1
          mountPath: /data/pvc1
        - name: pvc2
          mountPath: /data/pvc2
  volumes:
    - name: jhome
      nfs:
        path: /jhome
        server: nfs-jhome.tu.lsst.org
    - name: lsstdata
      nfs:
        path: /lsstdata
        server: nfs-lsstdata.tu.lsst.org
    - name: project
      nfs:
        path: /project
        server: nfs-project.tu.lsst.org
    - name: scratch
      nfs:
        path: /scratch
        server: nfs-scratch.tu.lsst.org
    - name: pvc1
      persistentVolumeClaim:
        claimName: ceph-demo-all-pvc1
    - name: pvc2
      persistentVolumeClaim:
        claimName: ceph-demo-all-pvc2
      #csi:
      #  driver: nfs.csi.k8s.io
      #  volumeAttributes:
      #    server: rook-ceph-nfs-jhome-a.rook-ceph.svc.cluster.local
      #    share: /jhome
